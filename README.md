# Home_Sales
In this project, we utilized SparkSQL for a comprehensive analysis of home sales data. Initially, we created a new GitHub repository named Home_Sales, cloned it locally, and renamed the provided starter file to Home_Sales.ipynb. Essential PySpark SQL functions were imported, and a Spark session was initiated. We then read the home_sales_revised.csv file into a Spark DataFrame and created a temporary view named home_sales to enable SQL queries. The analysis involved calculating the average price for four-bedroom houses for each year, the average price for three-bedroom, three-bathroom houses for each year they were built, and the average price for houses with three bedrooms, three bathrooms, two floors, and at least 2,000 square feet for each year. Additionally, we determined the average price per view rating for homes with an average price of at least $350,000, measuring the query runtime. To optimize performance, we cached the home_sales table, verified the caching, and reran the query to compare runtimes between cached and uncached data. We further partitioned the data by the date_built field, stored it in Parquet format, and created a temporary view for the partitioned data. The query was executed again on the partitioned data to measure runtime. Finally, we uncached the home_sales table and verified the uncaching. The completed notebook was saved and pushed to the Home_Sales GitHub repository. This structured approach ensured efficient data analysis and management using SparkSQL, leveraging caching and partitioning techniques to optimize performance.
